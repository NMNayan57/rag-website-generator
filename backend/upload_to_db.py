"""
Phase 2: Upload components from embeddings.json to PostgreSQL
"""

import json
import psycopg2
from psycopg2.extras import execute_values
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

print("üì§ Uploading Components to Database")
print("=" * 80 + "\n")

# Get database connection
DATABASE_URL = os.getenv('DATABASE_URL')

if not DATABASE_URL:
    print("‚ùå ERROR: DATABASE_URL not found in .env file")
    exit(1)

# Load embeddings
print("üìÇ Loading embeddings.json...")
try:
    with open('./embeddings.json', 'r', encoding='utf-8') as f:
        components = json.load(f)
    print(f"‚úÖ Loaded {len(components)} components\n")
except FileNotFoundError:
    print("‚ùå ERROR: embeddings.json not found")
    print("   Run embed_components.py first!")
    exit(1)

# Connect to database
print("üì° Connecting to database...")
try:
    conn = psycopg2.connect(DATABASE_URL)
    cursor = conn.cursor()
    print("‚úÖ Connected!\n")
except Exception as e:
    print(f"‚ùå Connection failed: {e}")
    exit(1)

# Prepare data for bulk insert
print("üîÑ Preparing data for upload...")
insert_data = []

for comp in components:
    insert_data.append((
        comp['id'],
        comp['filename'],
        comp['category'],
        comp['style_tags'],
        comp['color_scheme'],
        comp['complexity'],
        json.dumps(comp['props_schema']),  # Convert dict to JSON
        comp['dependencies'],
        comp['description'],
        comp['source'],
        comp['code'],
        comp['embedding']  # Already a list
    ))

print(f"‚úÖ Prepared {len(insert_data)} records\n")

# Upload to database
print("üì§ Uploading to database...")
try:
    # Use execute_values for efficient bulk insert
    execute_values(
        cursor,
        """
        INSERT INTO components (
            id, filename, category, style_tags, color_scheme, 
            complexity, props_schema, dependencies, description, 
            source, code, embedding
        ) VALUES %s
        ON CONFLICT (id) DO UPDATE SET
            filename = EXCLUDED.filename,
            category = EXCLUDED.category,
            style_tags = EXCLUDED.style_tags,
            color_scheme = EXCLUDED.color_scheme,
            complexity = EXCLUDED.complexity,
            props_schema = EXCLUDED.props_schema,
            dependencies = EXCLUDED.dependencies,
            description = EXCLUDED.description,
            source = EXCLUDED.source,
            code = EXCLUDED.code,
            embedding = EXCLUDED.embedding,
            updated_at = NOW()
        """,
        insert_data
    )
    
    conn.commit()
    print(f"‚úÖ Uploaded {len(insert_data)} components successfully!\n")
    
except Exception as e:
    print(f"‚ùå Upload failed: {e}")
    conn.rollback()
    cursor.close()
    conn.close()
    exit(1)

# Verify upload
print("üîç Verifying upload...")
cursor.execute("SELECT COUNT(*) FROM components;")
count = cursor.fetchone()[0]
print(f"‚úÖ Database contains {count} components\n")

# Show sample data
print("üìä Sample data from database:")
cursor.execute("""
    SELECT id, category, array_length(style_tags, 1) as num_tags 
    FROM components 
    LIMIT 5;
""")

results = cursor.fetchall()
print("\n   ID                          | Category    | Tags")
print("   " + "-" * 55)
for row in results:
    print(f"   {row[0]:30} | {row[1]:11} | {row[2]}")

# Test vector search
print("\nüß™ Testing vector search...")
cursor.execute("""
    SELECT id, category, 1 - (embedding <=> embedding) as self_similarity
    FROM components
    LIMIT 1;
""")
test = cursor.fetchone()
print(f"‚úÖ Vector search working! (self-similarity: {test[2]:.3f})")

# Close connection
cursor.close()
conn.close()

print("\n" + "=" * 80)
print("üéâ SUCCESS! All components uploaded to database!")
print("=" * 80)
print("\nüìä Summary:")
print(f"   ‚Ä¢ Total components: {count}")
print(f"   ‚Ä¢ Vector embeddings: ‚úÖ Working")
print(f"   ‚Ä¢ Indexes: ‚úÖ Created")
print(f"   ‚Ä¢ Search function: ‚úÖ Ready")
print("\nüéØ Next step: Run test_db_search.py to test database retrieval")